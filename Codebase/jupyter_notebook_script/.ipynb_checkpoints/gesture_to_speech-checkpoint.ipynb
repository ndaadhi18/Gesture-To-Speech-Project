{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6b218b-ab30-48d7-8e5a-4fd4e46cb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MediaPipe for hand tracking\n",
    "import mediapipe as mp\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Text-to-Speech\n",
    "from gtts import gTTS\n",
    "\n",
    "# Jupyter display for audio\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80745fc5-be70-462f-bf73-54eda9e82699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(image_path):\n",
    "    \"\"\"\n",
    "    Extracts 21 hand landmarks (x, y, z) from an image using MediaPipe.\n",
    "    Returns a flat numpy array of shape (63,) or None if no hand is detected.\n",
    "    \"\"\"\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    hands.close()\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for lm in results.multi_hand_landmarks[0].landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        return np.array(landmarks)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e8d3c5d-ebf9-4a6a-a6d0-3504c5915f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks shape: (63,)\n",
      "Landmarks: [ 4.64867830e-01  8.80217552e-01  1.02489582e-06  3.71870458e-01\n",
      "  8.01113844e-01 -8.77911672e-02  3.57219130e-01  6.71855748e-01\n",
      " -1.33669019e-01  4.62136000e-01  5.76732516e-01 -1.76605433e-01\n",
      "  5.60061991e-01  5.09061694e-01 -2.15091363e-01  3.70272011e-01\n",
      "  4.61181670e-01 -6.88067228e-02  3.37009907e-01  2.83874810e-01\n",
      " -1.28876448e-01  3.21013629e-01  1.70809790e-01 -1.70739815e-01\n",
      "  3.10772389e-01  7.96734393e-02 -1.99820369e-01  4.76568371e-01\n",
      "  4.64770943e-01 -7.29502887e-02  5.14914870e-01  2.69875973e-01\n",
      " -1.33014694e-01  5.32648325e-01  1.52956322e-01 -1.80291295e-01\n",
      "  5.48952460e-01  5.55894375e-02 -2.09491700e-01  5.57952702e-01\n",
      "  5.20417094e-01 -8.78244787e-02  6.14293694e-01  4.01765853e-01\n",
      " -1.78209692e-01  5.65443397e-01  4.80156362e-01 -2.05650017e-01\n",
      "  5.29094040e-01  5.44319272e-01 -2.02927113e-01  6.19916260e-01\n",
      "  6.02885008e-01 -1.08538456e-01  6.37440145e-01  5.20595968e-01\n",
      " -1.84245676e-01  5.83595812e-01  5.74259400e-01 -1.93148509e-01\n",
      "  5.41411281e-01  6.28585279e-01 -1.84954032e-01]\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = '../data/indian_sign_language/Indian/2/0.jpg'  # Update path as needed\n",
    "landmarks = extract_hand_landmarks(sample_image_path)\n",
    "print(\"Landmarks shape:\", landmarks.shape if landmarks is not None else \"No hand detected\")\n",
    "print(\"Landmarks:\", landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e634471d-7ad8-4a6c-8b1e-207089beddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def process_dataset(dataset_root):\n",
    "    \"\"\"\n",
    "    Processes all images in the dataset, extracts landmarks, and saves features and labels as .npy files.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(dataset_root))\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "    print(\"Class mapping:\", class_to_idx)\n",
    "    \n",
    "    for cls_name in class_names:\n",
    "        cls_folder = os.path.join(dataset_root, cls_name)\n",
    "        image_paths = glob.glob(os.path.join(cls_folder, \"*.jpg\"))\n",
    "        print(f\"Processing {cls_name} ({len(image_paths)} images)\")\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        for img_path in tqdm(image_paths):\n",
    "            lm = extract_hand_landmarks(img_path)\n",
    "            if lm is not None:\n",
    "                features.append(lm)\n",
    "                labels.append(class_to_idx[cls_name])\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "                if fail_count < 5:  # Print first few failures\n",
    "                    print(f\"Failed to detect hand in: {img_path}\")\n",
    "        print(f\"Success: {success_count}, Failed: {fail_count}\")\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Total samples: {features.shape[0]}\")\n",
    "    # Save features and labels\n",
    "    np.save(os.path.join(dataset_root, \"features.npy\"), features)\n",
    "    np.save(os.path.join(dataset_root, \"labels.npy\"), labels)\n",
    "    print(\"Saved features and labels as .npy files.\")\n",
    "    return class_to_idx\n",
    "    # Add this to your process_dataset function to debug\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257e3bd-5cc0-4a7d-b881-f1c7ac942fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for Indian Sign Language dataset: Already done\n",
    "dataset_root = \"../data/indian_sign_language/Indian\"  # Adjust path if needed\n",
    "class_to_idx = process_dataset(dataset_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92dfd8de-d694-456d-b648-eabd6bea3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing custom gestures dataset...\n",
      "Class mapping: {'Bye': 0, 'Hello': 1, 'No': 2, 'Perfect': 3, 'Thank You': 4, 'Yes': 5}\n",
      "Processing Bye (400 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 1/400 [00:00<00:40,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Bye\\Image_1667239052.2083564.jpg\n",
      "Failed to detect hand in: ../data/gestures_dataset\\Bye\\Image_1667239052.8112261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 3/400 [00:00<00:38, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Bye\\Image_1667239052.8485913.jpg\n",
      "Failed to detect hand in: ../data/gestures_dataset\\Bye\\Image_1667239052.882727.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:55<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0, Failed: 400\n",
      "Processing Hello (400 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 3/400 [00:00<00:59,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Hello\\Image_1667238913.6247861.jpg\n",
      "Failed to detect hand in: ../data/gestures_dataset\\Hello\\Image_1667238913.6402302.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                 | 5/400 [00:00<00:56,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Hello\\Image_1667238913.6956959.jpg\n",
      "Failed to detect hand in: ../data/gestures_dataset\\Hello\\Image_1667238913.715463.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:53<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 7, Failed: 393\n",
      "Processing No (400 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 1/400 [00:00<00:51,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\No\\Image_1667239349.9046743.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 2/400 [00:00<01:03,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\No\\Image_1667239350.5289142.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 3/400 [00:00<00:59,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\No\\Image_1667239350.576339.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 4/400 [00:00<00:55,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\No\\Image_1667239350.6674688.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:57<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0, Failed: 400\n",
      "Processing Perfect (400 images)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 1/400 [00:00<01:03,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Perfect\\Image_1667239499.214726.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 3/400 [00:00<01:07,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Perfect\\Image_1667239499.8150597.jpg\n",
      "Failed to detect hand in: ../data/gestures_dataset\\Perfect\\Image_1667239499.8476486.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                 | 5/400 [00:00<00:57,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect hand in: ../data/gestures_dataset\\Perfect\\Image_1667239499.8863125.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▋                                                                       | 48/400 [00:06<00:50,  6.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m gestures_dataset_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/gestures_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Adjust path if needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing custom gestures dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m gestures_class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgestures_dataset_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(dataset_root)\u001b[0m\n\u001b[0;32m     18\u001b[0m fail_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m tqdm(image_paths):\n\u001b[1;32m---> 20\u001b[0m     lm \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hand_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(lm)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mextract_hand_landmarks\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m hands\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\project_env\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\project_env\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process the gestures_dataset folder\n",
    "gestures_dataset_root = \"../data/gestures_dataset\"  # Adjust path if needed\n",
    "print(\"Processing custom gestures dataset...\")\n",
    "gestures_class_to_idx = process_dataset(gestures_dataset_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f69f879b-0ab0-4cd6-aeb5-48c2f6f54073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_landmarks_for_cnn(landmarks, grid_size=7):\n",
    "    \"\"\"\n",
    "    Reshape the 1D landmark array (63 values) into a 2D grid format suitable for CNN processing.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: numpy array of shape (63,) containing x,y,z coordinates of 21 hand landmarks\n",
    "        grid_size: size of the grid to reshape landmarks into (default: 7x7)\n",
    "    \n",
    "    Returns:\n",
    "        3-channel image-like representation of landmarks with shape (3, grid_size, grid_size)\n",
    "    \"\"\"\n",
    "    # Reshape the 63 values (21 landmarks x 3 coordinates) into 3 channels\n",
    "    x_coords = landmarks[0::3]  # x coordinates\n",
    "    y_coords = landmarks[1::3]  # y coordinates\n",
    "    z_coords = landmarks[2::3]  # z coordinates\n",
    "    \n",
    "    # Create a spatial representation by placing landmarks in a grid\n",
    "    x_channel = np.zeros((grid_size, grid_size))\n",
    "    y_channel = np.zeros((grid_size, grid_size))\n",
    "    z_channel = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    # Map the 21 landmarks to positions in the grid\n",
    "    for i in range(21):\n",
    "        # Scale coordinates to grid indices\n",
    "        x_idx = min(int(x_coords[i] * (grid_size-1)), grid_size-1)\n",
    "        y_idx = min(int(y_coords[i] * (grid_size-1)), grid_size-1)\n",
    "        \n",
    "        # Set the values in the grid\n",
    "        x_channel[y_idx, x_idx] = x_coords[i]\n",
    "        y_channel[y_idx, x_idx] = y_coords[i]\n",
    "        z_channel[y_idx, x_idx] = z_coords[i]\n",
    "    \n",
    "    # Stack channels to create a 3-channel representation\n",
    "    grid_representation = np.stack([x_channel, y_channel, z_channel], axis=0)\n",
    "    return grid_representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c927a489-8400-4974-bab9-459b8213e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (63,)\n",
      "Reshaped for CNN: (3, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Reshape a single landmark sample\n",
    "sample_landmark = np.load(\"../data/indian_sign_language/Indian/features.npy\")[0]\n",
    "reshaped_sample = reshape_landmarks_for_cnn(sample_landmark)\n",
    "print(\"Original shape:\", sample_landmark.shape)\n",
    "print(\"Reshaped for CNN:\", reshaped_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "032b8a72-dd93-4e6f-8aa4-209bf87a3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the entire dataset\n",
    "def preprocess_dataset_for_cnn(features_path, labels_path, grid_size=7):\n",
    "    \"\"\"\n",
    "    Preprocess the entire dataset for CNN training.\n",
    "    \n",
    "    Args:\n",
    "        features_path: path to the features.npy file\n",
    "        labels_path: path to the labels.npy file\n",
    "        grid_size: size of the grid for reshaping\n",
    "        \n",
    "    Returns:\n",
    "        X: reshaped features suitable for CNN\n",
    "        y: labels\n",
    "    \"\"\"\n",
    "    features = np.load(features_path)\n",
    "    labels = np.load(labels_path)\n",
    "    \n",
    "    # Reshape all samples\n",
    "    X = np.zeros((len(features), 3, grid_size, grid_size))\n",
    "    for i in range(len(features)):\n",
    "        X[i] = reshape_landmarks_for_cnn(features[i], grid_size)\n",
    "    \n",
    "    return X, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f8865ba-0fb6-4b19-ba7a-2840e870d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureCNN(nn.Module):\n",
    "    def __init__(self, num_classes, grid_size=7):\n",
    "        super(GestureCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # For a 7x7 input with 3 max pooling layers (each dividing by 2), the output size is 7/(2^3) ≈ 1\n",
    "        conv_output_size = max(1, grid_size // (2**3))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * conv_output_size * conv_output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13a73bbc-1ac9-441f-ab28-24023337221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN dataset class\n",
    "class CNNLandmarkDataset(Dataset):\n",
    "    \"\"\"Dataset for CNN-ready hand landmark features and labels.\"\"\"\n",
    "    def __init__(self, features_path, labels_path, grid_size=7, transform=None):\n",
    "        features = np.load(features_path)\n",
    "        self.labels = np.load(labels_path)\n",
    "        \n",
    "        # Reshape features for CNN\n",
    "        self.features = np.zeros((len(features), 3, grid_size, grid_size))\n",
    "        for i in range(len(features)):\n",
    "            self.features[i] = reshape_landmarks_for_cnn(features[i], grid_size)\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "            \n",
    "        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a5fd883-237b-481c-adaf-b73715cb441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Load the dataset\n",
    "grid_size = 7  # Size of the grid for reshaping landmarks\n",
    "isl_features_path = \"../data/indian_sign_language/Indian/features.npy\"\n",
    "isl_labels_path = \"../data/indian_sign_language/Indian/labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f68ff3c0-f727-4ee5-a945-8a5fc583cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Dataset size: 41684\n",
      "Feature shape: torch.Size([3, 7, 7])\n",
      "Number of classes: 35\n",
      "Using device: cuda\n",
      "GestureCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=35, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create CNN dataset\n",
    "isl_cnn_dataset = CNNLandmarkDataset(isl_features_path, isl_labels_path, grid_size)\n",
    "\n",
    "# Check dataset\n",
    "print(f\"CNN Dataset size: {len(isl_cnn_dataset)}\")\n",
    "print(f\"Feature shape: {isl_cnn_dataset[0][0].shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(np.load(isl_labels_path)))}\")\n",
    "\n",
    "# Initialize the CNN model\n",
    "num_classes = len(np.unique(np.load(isl_labels_path)))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "cnn_model = GestureCNN(num_classes=num_classes, grid_size=grid_size)\n",
    "cnn_model = cnn_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9034f061-7bf7-485a-aa39-0c531bd51556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saving if it doesn't exist\n",
    "os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "\n",
    "# Save the current state of your model and preprocessing\n",
    "torch.save({\n",
    "    'model_state_dict': cnn_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'grid_size': grid_size,\n",
    "    'num_classes': num_classes\n",
    "}, \"../checkpoints/model_checkpoint.pth\")\n",
    "\n",
    "# You can also save any important variables\n",
    "import pickle\n",
    "with open(\"../checkpoints/variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"class_to_idx\": class_to_idx,\n",
    "        \"gestures_class_to_idx\": gestures_class_to_idx\n",
    "        # Add any other variables you want to save\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a518258-9a0e-4bfe-87e7-71e88e151a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved successfully! You can continue from this point tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Create directory for checkpoints\n",
    "os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "\n",
    "# Save important variables and mappings\n",
    "checkpoint_data = {\n",
    "    'isl_class_to_idx': class_to_idx,  # Rename for clarity\n",
    "    'gestures_class_to_idx': gestures_class_to_idx,  # Add this line\n",
    "    'dataset_processed': True,\n",
    "    'last_completed_step': 'dataset_processing'\n",
    "}\n",
    "\n",
    "\n",
    "# Save the checkpoint\n",
    "import pickle\n",
    "with open(\"../checkpoints/progress_checkpoint.pkl\", \"wb\") as f:\n",
    "    pickle.dump(checkpoint_data, f)\n",
    "\n",
    "print(\"Progress saved successfully! You can continue from this point tomorrow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445b6965-55ed-4afb-8e3e-0699531ef96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous session state restored successfully!\n",
      "Indian Sign Language classes: 35\n",
      "Custom Gestures classes: 6\n",
      "Indian Sign Language processed data available: True\n",
      "Custom Gestures processed data available: True\n",
      "ISL Features shape: (5, 63)\n",
      "ISL Labels sample: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if checkpoint exists and load it\n",
    "checkpoint_path = \"../checkpoints/progress_checkpoint.pkl\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    \n",
    "    # Restore variables\n",
    "    isl_class_to_idx = checkpoint_data.get('isl_class_to_idx')\n",
    "    gestures_class_to_idx = checkpoint_data.get('gestures_class_to_idx')\n",
    "    \n",
    "    print(\"Previous session state restored successfully!\")\n",
    "    print(f\"Indian Sign Language classes: {len(isl_class_to_idx) if isl_class_to_idx else 0}\")\n",
    "    print(f\"Custom Gestures classes: {len(gestures_class_to_idx) if gestures_class_to_idx else 0}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. You'll need to rerun the preprocessing steps.\")\n",
    "\n",
    "# Verify that the processed data files exist\n",
    "isl_features_path = \"../data/indian_sign_language/Indian/features.npy\"\n",
    "isl_labels_path = \"../data/indian_sign_language/Indian/labels.npy\"\n",
    "gestures_features_path = \"../data/gestures_dataset/features.npy\"\n",
    "gestures_labels_path = \"../data/gestures_dataset/labels.npy\"\n",
    "\n",
    "# Check if files exist\n",
    "isl_data_exists = os.path.exists(isl_features_path) and os.path.exists(isl_labels_path)\n",
    "gestures_data_exists = os.path.exists(gestures_features_path) and os.path.exists(gestures_labels_path)\n",
    "\n",
    "print(f\"Indian Sign Language processed data available: {isl_data_exists}\")\n",
    "print(f\"Custom Gestures processed data available: {gestures_data_exists}\")\n",
    "\n",
    "# If data exists, load sample to verify\n",
    "if isl_data_exists:\n",
    "    # Load a small sample to verify data integrity\n",
    "    features_sample = np.load(isl_features_path)[:5]\n",
    "    labels_sample = np.load(isl_labels_path)[:5]\n",
    "    print(f\"ISL Features shape: {features_sample.shape}\")\n",
    "    print(f\"ISL Labels sample: {labels_sample}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfd3795-efc3-4751-9ac3-4e41da2e1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISL dataset: 41684 samples\n",
      "Gestures dataset: 7 samples\n",
      "Combined dataset: 41691 samples\n",
      "Total number of classes: 41\n"
     ]
    }
   ],
   "source": [
    "# Load both preprocessed datasets and combine them\n",
    "def combine_preprocessed_datasets():\n",
    "    # Load Indian Sign Language dataset\n",
    "    isl_features_path = \"../data/indian_sign_language/Indian/features.npy\"\n",
    "    isl_labels_path = \"../data/indian_sign_language/Indian/labels.npy\"\n",
    "    isl_features = np.load(isl_features_path)\n",
    "    isl_labels = np.load(isl_labels_path)\n",
    "    \n",
    "    # Load custom gestures dataset\n",
    "    gestures_features_path = \"../data/gestures_dataset/features.npy\"\n",
    "    gestures_labels_path = \"../data/gestures_dataset/labels.npy\"\n",
    "    gestures_features = np.load(gestures_features_path)\n",
    "    gestures_labels = np.load(gestures_labels_path)\n",
    "    \n",
    "    # Load class mappings\n",
    "    with open(\"../checkpoints/progress_checkpoint.pkl\", \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    \n",
    "    isl_class_to_idx = checkpoint_data['isl_class_to_idx']\n",
    "    gestures_class_to_idx = checkpoint_data['gestures_class_to_idx']\n",
    "    \n",
    "    # Create a unified class mapping\n",
    "    # First, include all ISL classes\n",
    "    unified_class_to_idx = isl_class_to_idx.copy()\n",
    "    \n",
    "    # Then add gesture classes with offset\n",
    "    offset = len(isl_class_to_idx)\n",
    "    for gesture_class, idx in gestures_class_to_idx.items():\n",
    "        unified_class_to_idx[f\"gesture_{gesture_class}\"] = idx + offset\n",
    "    \n",
    "    # Adjust gesture labels to avoid overlap with ISL labels\n",
    "    adjusted_gestures_labels = gestures_labels + offset\n",
    "    \n",
    "    # Combine features and adjusted labels\n",
    "    combined_features = np.vstack((isl_features, gestures_features))\n",
    "    combined_labels = np.concatenate((isl_labels, adjusted_gestures_labels))\n",
    "    \n",
    "    print(f\"ISL dataset: {len(isl_labels)} samples\")\n",
    "    print(f\"Gestures dataset: {len(gestures_labels)} samples\")\n",
    "    print(f\"Combined dataset: {len(combined_labels)} samples\")\n",
    "    print(f\"Total number of classes: {len(unified_class_to_idx)}\")\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    os.makedirs(\"../data/combined_dataset\", exist_ok=True)\n",
    "    np.save(\"../data/combined_dataset/features.npy\", combined_features)\n",
    "    np.save(\"../data/combined_dataset/labels.npy\", combined_labels)\n",
    "    \n",
    "    # Save the unified class mapping\n",
    "    with open(\"../data/combined_dataset/class_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(unified_class_to_idx, f)\n",
    "    \n",
    "    return combined_features, combined_labels, unified_class_to_idx\n",
    "\n",
    "# Execute the function to combine datasets\n",
    "combined_features, combined_labels, unified_class_to_idx = combine_preprocessed_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d8213-44fd-45a4-9e29-ec55a8236fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
